[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Juho Timonen",
    "section": "",
    "text": "I completed my PhD thesis Bayesian Ordinary Differential Equation and Gaussian Process Modeling of Biomedical Data in the Computational Systems Biology group of Aalto University, Department of Computer Science. I work with Generable Inc., employing probabilistic models to support decision-making in drug development.\n\n\n\nNeulamäki (Kuopio, Finland)"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Blog posts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe odemodeling R package\n\n\n\n\n\n\nR\n\n\nOrdinary differential equations\n\n\nStan\n\n\n\nDemonstrating core functionality of the package.\n\n\n\n\n\nSep 20, 2024\n\n\nJuho Timonen\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Stan codebase - Part 2: Samplers\n\n\n\n\n\n\nStan\n\n\nC++\n\n\n\nStudying the C++ code of the NUTS algorithm of Stan.\n\n\n\n\n\nJan 14, 2022\n\n\nJuho Timonen\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Stan codebase - Part 1: Finding an entry point\n\n\n\n\n\n\nStan\n\n\nC++\n\n\n\nOverview of the different libraries related to Stan and their organization, and finding and entry point to the internal C++ code.\n\n\n\n\n\nNov 29, 2021\n\n\nJuho Timonen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Scalable mixed-domain Gaussian process modeling and model reduction for longitudinal data. Timonen, J. & Lähdesmäki, H., 2025, Bayesian Analysis (to appear)\nAn importance sampling approach for reliable and efficient inference in Bayesian ordinary differential equation models. Timonen, J., Siccha, N., Bales, B., Lähdesmäki, H. & Vehtari, A., 2023, Stat. 12, 1, e614.\nlgpr: an interpretable non-parametric method for inferring covariate effects from longitudinal data. Timonen, J., Mannerström, H., Vehtari, A. & Lähdesmäki, H., 2021, Bioinformatics. 37, 13, p. 1860-1867.\nA probabilistic framework for molecular network structure inference by means of mechanistic modeling. Timonen, J., Mannerström, H., Lähdesmäki, H. & Intosalmi, J., 2018, IEEE-ACM Transactions on Computational Biology and Bioinformatics. 16, 6, p. 1843-1854"
  },
  {
    "objectID": "posts/post-01/index.html",
    "href": "posts/post-01/index.html",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "",
    "text": "So, you have your Stan model written and are doing inference for it, but something weird is happening? Or maybe you want to extend Stan but don’t know where to start because the source code repositories look daunting. These are some of the possible reasons why someone might want to study the internals of Stan, and what is happening under the hood. I have for various reasons for a long time wanted to just see what is happening line-by-line. In this post, I am going to look at how a typical program execution starts to travel through all the different libraries related to Stan, using CmdStanR as the starting point.\n\nRelationships between different libraries and various interfaces related to Stan are visualized in the above diagram. The C++ core that we study in this post is organized into three parts.\n\nCmdStan: A command-line interface to Stan\nStan: The MCMC and optimization algorithms\nStan Math: Mathematical functions and their gradients (automatic differentiation)\n\nMany higher-level interfaces, like CmdStanR and CmdStanPy, call CmdStan internally. RStan and PyStan employ different strategies that do not rely on CmdStan. A benefit of CmdStan is that it is always released simultaneously with Stan with the same version number, which means that CmdStan is always up to date. In this, post we study the most recent Stan version 2.28.2, and if the source code structure doesn’t experience dramatic changes in the near future, this post might stay relevant for future versions too."
  },
  {
    "objectID": "posts/post-01/index.html#creating-the-executable",
    "href": "posts/post-01/index.html#creating-the-executable",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "2.1 Creating the executable",
    "text": "2.1 Creating the executable\nThe first thing we look at is cmdstan_model(stan_file = \"mymodel.stan\"). This does two interesting things.\n\nTranspiles the Stan model to C++ code using stanc3\nCompiles the C++ code into an executable file mymodel.exe (without the .exe file suffix on Mac or Linux).\n\nWe could have used model$save_hpp_file() to save the model C++ code into mymodel.hpp if we wanted to look at that. However, we are now interested in the C++ code that doesn’t depend on the model. I would imagine that also a lot of this model-independent code has to go into the executable."
  },
  {
    "objectID": "posts/post-01/index.html#running-the-executable",
    "href": "posts/post-01/index.html#running-the-executable",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "2.2 Running the executable",
    "text": "2.2 Running the executable\nThe call model$sample(adapt_delta = 0.95, refresh = 100) creates four processes (because the default number of chains is four) that each run the executable. For example, the first process creates the command-line call\nmymodel.exe id=1 random seed=660816326 output file=&lt;opath&gt;.csv refresh=100 profile_file=&lt;ppath&gt;.csv method=sample save_warmup=0 algorithm=hmc engine=nuts adapt delta=0.95 engaged=1\nwhere &lt;opath&gt; and &lt;ppath&gt; are paths to some temporary CSV files on the computer. Arguments delta=0.95 and refresh=100 are things that we specified and others are defaults created by CmdStanR. You can find explanations for the command-line arguments in the CmdStan User’s Guide.\nFor other processes the id argument is 2, 3, and 4. From now on we study only one chain (the one with id=1) and next try to find the entry point in the CmdStan code that is started with the above command-line instruction."
  },
  {
    "objectID": "posts/post-01/index.html#main.cpp",
    "href": "posts/post-01/index.html#main.cpp",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "3.1 main.cpp",
    "text": "3.1 main.cpp\nWe find a main.cpp, which looks promising. It actually includes an\nint main(int argc, const char *argv[]) {\n  // ...\n}\nfunction which is the starting point of any C++ program. Based on our command line arguments, at this point argc (number of command line arguments) should be 15, argv[0] should be \"mymodel.exe\", argv[1] should be \"id=1\" and so on. We see that main just calls cmdstan::command(argc, argv), which is defined in command.hpp."
  },
  {
    "objectID": "posts/post-01/index.html#command.hpp",
    "href": "posts/post-01/index.html#command.hpp",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "3.2 command.hpp",
    "text": "3.2 command.hpp\nInside the command() function is a huge if-else parade which is quite difficult to read. So here is a high-level summary of the control flow inside it.\nint command(int argc, const char *argv[]) {\n\n  // ... parse the arguments\n  // ... initialize model\n  // ... initialize writers\n\n  if (user_method-&gt;arg(\"generate_quantities\")) {\n    // ...\n  } else if (user_method-&gt;arg(\"diagnose\")) {\n    // ...\n  } else if (user_method-&gt;arg(\"optimize\")) {\n    // ...\n  } else if (user_method-&gt;arg(\"sample\")) {\n    // ...\n    if (model.num_params_r() == 0 || algo-&gt;value() == \"fixed_param\") {\n      // ...\n    } else if (algo-&gt;value() == \"hmc\") {\n      // ...\n      if (adapt_engaged == true && num_warmup == 0) {\n        // ... error\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == false && metric_supplied == false) {\n        // ... \n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == false && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == true && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == true && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == false && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == false && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == true && metric_supplied == false) {\n        // ... WE END UP HERE\n        return_code = stan::services::sample::hmc_nuts_diag_e_adapt(\n            model, num_chains, init_contexts, random_seed, id, init_radius,\n            num_warmup, num_samples, num_thin, save_warmup, refresh, stepsize,\n            stepsize_jitter, max_depth, delta, gamma, kappa, t0, init_buffer,\n            term_buffer, window, interrupt, logger, init_writers,\n            sample_writers, diagnostic_writers);\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == true && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"unit_e\"\n                 && adapt_engaged == false) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"unit_e\"\n                 && adapt_engaged == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == false && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == false && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == true && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == true && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == false && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == false && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == true && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == true && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"unit_e\"\n                 && adapt_engaged == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"unit_e\"\n                 && adapt_engaged == true) {\n        // ...\n      }\n    }\n  } else if (user_method-&gt;arg(\"variational\")) {\n    // ...\n  }\n  // ...\n  return return_code;\n}\nIn most of the branches, the left-out part // ... ends up calling something from stan::services. This is also the case in our example, and because our method argument is sample,the default algorithm is NUTS with adaptation engaged and the default metric is diagonal (and we haven’t supplied the metric), we will call stan::services::sample::hmc_nuts_diag_e_adapt(). We will therefore now jump from CmdStan to Stan. Hooray!"
  },
  {
    "objectID": "posts/post-01/index.html#hmc_nuts_diag_e_adapt.hpp",
    "href": "posts/post-01/index.html#hmc_nuts_diag_e_adapt.hpp",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "4.1 hmc_nuts_diag_e_adapt.hpp",
    "text": "4.1 hmc_nuts_diag_e_adapt.hpp\nIn services/sample we find hmc_nuts_diag_e_adapt.hpp which contains the function that we called from CmdStan. But wait, it is actually overloaded with four different versions of it with the same name. Not to mention each of these are templated. We will not stop here to think much about why these all versions of hmc_nuts_diag_e_adapt() are needed. A very valid print debugging approach reveals that in our case, we call the fourth one, which calls the second one, which then calls the first one. There we have\nstd::vector&lt;double&gt; cont_vector = util::initialize(\n      model, init, rng, init_radius, true, logger, init_writer);\nwhere the parameter values are initialized. In initialize(), which is defined in initialize.hpp, we try most 100 random initial points, until a point where log probability and its gradient can be evaluated successfully. In our case the first try is successful. Therefore we can now think that we exist somewhere in the (unconstrained) parameter space, at a point stored in cont_vector. The next step is to call\n  util::run_adaptive_sampler(\n      sampler, model, cont_vector, num_warmup, num_samples, num_thin, refresh,\n      save_warmup, rng, interrupt, logger, sample_writer, diagnostic_writer);\nwhich we will look at next."
  },
  {
    "objectID": "posts/post-01/index.html#run_adaptive_sampler.hpp",
    "href": "posts/post-01/index.html#run_adaptive_sampler.hpp",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "4.2 run_adaptive_sampler.hpp",
    "text": "4.2 run_adaptive_sampler.hpp\nSo we are now at run_adaptive_sampler.hpp which is in services/util. There we have three interesting parts.\n\n\nInitializing stepsize\n\n\nGenerating transitions, adaptation engaged (warmup)\n\n\nGenerating transitions, adaptation disengaged (sampling)\n\n\nThe part\nsampler.init_stepsize(logger)\ninitializes the stepsize and is defined in mcmc/hmc/base_hmc.hpp. This already involves a bit of Hamiltonian computations and evolving the Leapfrog integrator. After this, we start to actually generate MCMC transitions using the sampler. This is done in two phases with calls to\n  util::generate_transitions();\nand in the first one we have adaptation engaged. We will look at generate_transitions() in the next blog post. Spoiler alert: sampler.init_stepsize() will be called there again so we will also look at it more."
  },
  {
    "objectID": "posts/post-02/index.html",
    "href": "posts/post-02/index.html",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "",
    "text": "We pick up from where we left off in Part 1. We found out that CmdStan calls the Stan services in cmdstan::command(). For example with the command-line call\nmymodel.exe id=1 method=sample algorithm=hmc engine=nuts adapt engaged=1\n\nthe called service is stan::services::sample::hmc_nuts_diag_adapt()\nwhich then calls stan::services::util::run_adaptive_sampler()\nwhich calls stan::services::util::generate_transitions().\n\nNote: All code pieces shown from now on in this post are adapted from the stan source code, licenced under the new BSD licence. Comments starting with ... indicate parts that have been left out from original source code. During writing of this post, the most recent Stan version is 2.28.2. The hyperlinks to source code cannot be guaranteed to work in the future, if the source repo organization is changed or files are renamed.\n\n\n\nWe find generate_transitions() in generate_transitions.hpp.\nvoid generate_transitions(stan::mcmc::base_mcmc& sampler, int num_iterations,\n                          ..., stan::mcmc::sample& init_s, Model& model, ...) {\n  for (int m = 0; m &lt; num_iterations; ++m) {\n    // ... callbacks and progress printing\n    init_s = sampler.transition(init_s, logger);\n    // ... writing to output\n  }\n}\nAmong other things, it takes as input the sampler, model, and initial point in parameter space. The function is basically just a loop that calls sampler.transition() repeatedly for num_iterations times. Therefore, all interesting algorithmic code and properties that define how sampling works, whether adaptation is performed etc, have to be included as part of sampler.\nThis is why we now jump from the stan::services namespace to stan::mcmc, where the different samplers and their transitions are defined."
  },
  {
    "objectID": "posts/post-02/index.html#recap-of-part-1",
    "href": "posts/post-02/index.html#recap-of-part-1",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "",
    "text": "We pick up from where we left off in Part 1. We found out that CmdStan calls the Stan services in cmdstan::command(). For example with the command-line call\nmymodel.exe id=1 method=sample algorithm=hmc engine=nuts adapt engaged=1\n\nthe called service is stan::services::sample::hmc_nuts_diag_adapt()\nwhich then calls stan::services::util::run_adaptive_sampler()\nwhich calls stan::services::util::generate_transitions().\n\nNote: All code pieces shown from now on in this post are adapted from the stan source code, licenced under the new BSD licence. Comments starting with ... indicate parts that have been left out from original source code. During writing of this post, the most recent Stan version is 2.28.2. The hyperlinks to source code cannot be guaranteed to work in the future, if the source repo organization is changed or files are renamed."
  },
  {
    "objectID": "posts/post-02/index.html#starting-point-for-part-2",
    "href": "posts/post-02/index.html#starting-point-for-part-2",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "",
    "text": "We find generate_transitions() in generate_transitions.hpp.\nvoid generate_transitions(stan::mcmc::base_mcmc& sampler, int num_iterations,\n                          ..., stan::mcmc::sample& init_s, Model& model, ...) {\n  for (int m = 0; m &lt; num_iterations; ++m) {\n    // ... callbacks and progress printing\n    init_s = sampler.transition(init_s, logger);\n    // ... writing to output\n  }\n}\nAmong other things, it takes as input the sampler, model, and initial point in parameter space. The function is basically just a loop that calls sampler.transition() repeatedly for num_iterations times. Therefore, all interesting algorithmic code and properties that define how sampling works, whether adaptation is performed etc, have to be included as part of sampler.\nThis is why we now jump from the stan::services namespace to stan::mcmc, where the different samplers and their transitions are defined."
  },
  {
    "objectID": "posts/post-02/index.html#base_mcmc",
    "href": "posts/post-02/index.html#base_mcmc",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.1 base_mcmc",
    "text": "2.1 base_mcmc\nThis is just an interface for all MCMC samplers, as it doesn’t contain any function bodies.\nclass base_mcmc {\n public:\n  base_mcmc() {}\n\n  virtual ~base_mcmc() {}\n\n  virtual sample transition(sample& init_sample, callbacks::logger& logger) = 0;\n\n  virtual void get_sampler_param_names(std::vector&lt;std::string&gt;& names) {}\n\n  virtual void get_sampler_params(std::vector&lt;double&gt;& values) {}\n\n  //... other virtual functions without body\n\n};\nThe class member functions are all virtual (except the constructor), meaning that deriving classes can override them. We see that transition() is pure virtual (declared with = 0), meaning that a deriving class must override it in order to be instantiable."
  },
  {
    "objectID": "posts/post-02/index.html#base_hmc",
    "href": "posts/post-02/index.html#base_hmc",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.2 base_hmc",
    "text": "2.2 base_hmc\nThis is a base for all Hamiltonian samplers, and derives from base_mcmc.\ntemplate &lt;class Model, template &lt;class, class&gt; class Hamiltonian,\n          template &lt;class&gt; class Integrator, class BaseRNG&gt;\nclass base_hmc : public base_mcmc {\n public:\n  base_hmc(const Model& model, BaseRNG& rng)\n      : base_mcmc(),\n        z_(model.num_params_r()),\n        integrator_(),\n        hamiltonian_(model),\n        rand_int_(rng),\n        rand_uniform_(rand_int_),\n        nom_epsilon_(0.1),\n        epsilon_(nom_epsilon_),\n        epsilon_jitter_(0.0) {}\n\n  // ...\n\n  void seed(const Eigen::VectorXd& q) { z_.q = q; }\n\n  void init_hamiltonian(callbacks::logger& logger) {\n    this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n  }\n\n  void init_stepsize(callbacks::logger& logger) {\n    ps_point z_init(this-&gt;z_);\n\n    // Skip initialization for extreme step sizes\n    if (this-&gt;nom_epsilon_ == 0 || this-&gt;nom_epsilon_ &gt; 1e7\n        || std::isnan(this-&gt;nom_epsilon_))\n      return;\n\n    this-&gt;hamiltonian_.sample_p(this-&gt;z_, this-&gt;rand_int_);\n    this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n\n    // Guaranteed to be finite if randomly initialized\n    double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n\n    this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,\n                             logger);\n\n    double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n    if (std::isnan(h))\n      h = std::numeric_limits&lt;double&gt;::infinity();\n\n    double delta_H = H0 - h;\n\n    int direction = delta_H &gt; std::log(0.8) ? 1 : -1;\n\n    while (1) {\n      this-&gt;z_.ps_point::operator=(z_init);\n\n      this-&gt;hamiltonian_.sample_p(this-&gt;z_, this-&gt;rand_int_);\n      this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n\n      double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n\n      this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,\n                               logger);\n\n      double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n      if (std::isnan(h))\n        h = std::numeric_limits&lt;double&gt;::infinity();\n\n      double delta_H = H0 - h;\n\n      if ((direction == 1) && !(delta_H &gt; std::log(0.8)))\n        break;\n      else if ((direction == -1) && !(delta_H &lt; std::log(0.8)))\n        break;\n      else\n        this-&gt;nom_epsilon_ = direction == 1 ? 2.0 * this-&gt;nom_epsilon_\n                                            : 0.5 * this-&gt;nom_epsilon_;\n\n      if (this-&gt;nom_epsilon_ &gt; 1e7)\n        throw std::runtime_error(\n            \"Posterior is improper. \"\n            \"Please check your model.\");\n      if (this-&gt;nom_epsilon_ == 0)\n        throw std::runtime_error(\n            \"No acceptably small step size could \"\n            \"be found. Perhaps the posterior is \"\n            \"not continuous?\");\n    }\n\n    this-&gt;z_.ps_point::operator=(z_init);\n  }\n\n  // ...\n\n  typename Hamiltonian&lt;Model, BaseRNG&gt;::PointType& z() { return z_; }\n\n  const typename Hamiltonian&lt;Model, BaseRNG&gt;::PointType& z() const noexcept {\n    return z_;\n  }\n\n  // ... setters and getters for the protected properties\n\n  void sample_stepsize() {\n    this-&gt;epsilon_ = this-&gt;nom_epsilon_;\n    if (this-&gt;epsilon_jitter_)\n      this-&gt;epsilon_\n          *= 1.0 + this-&gt;epsilon_jitter_ * (2.0 * this-&gt;rand_uniform_() - 1.0);\n  }\n\n protected:\n  typename Hamiltonian&lt;Model, BaseRNG&gt;::PointType z_;\n  Integrator&lt;Hamiltonian&lt;Model, BaseRNG&gt; &gt; integrator_;\n  Hamiltonian&lt;Model, BaseRNG&gt; hamiltonian_;\n\n  BaseRNG& rand_int_;\n\n  // Uniform(0, 1) RNG\n  boost::uniform_01&lt;BaseRNG&&gt; rand_uniform_;\n\n  double nom_epsilon_;\n  double epsilon_;\n  double epsilon_jitter_;\n};\nWe see that the class doesn’t implement transition(), so it is also an abstract class. What it does do is it defines some attributes that all Hamiltonian samplers need.\n\n2.2.1 class attributes\nThe attributes listed as protected describe the internal state of the sampler. All Hamiltonian samplers have these attributes, and the most interesting ones of them are\n\nz_: current state of the sampler (point in parameter space)\nintegrator_: numerical integrator used to simulate the Hamiltonian dynamics\nhamiltonian_: the Hamiltonian system\nepsilon_ / nom_epsilon_: step size of the integrator\n\n\n\n2.2.2 getters\nThe above are private attributes and should not be directly accessed from the outside. Instead, there are some getter methods that can be used, for example\n  typename Hamiltonian&lt;Model, BaseRNG&gt;::PointType& z() { return z_; }\nso one could use sampler.z() to get the current point in the (unconstrained) parameter space.\n\n\n2.2.3 init_stepsize()\nThe first actual algorithm that we encouter is the init_stepsize() method of the base_hmc class, defined in base_hmc.hpp. This defines how the integrator’s initial stepsize (possibly given by user) is refined. As we saw in Part 1, this gets called in stan::services::sample::hmc_nuts_diag_e_adapt() before any MCMC iterations are done.\nvoid init_stepsize(callbacks::logger& logger) {\n    ps_point z_init(this-&gt;z_);\n\n    // Skip initialization for extreme step sizes\n    if (this-&gt;nom_epsilon_ == 0 || this-&gt;nom_epsilon_ &gt; 1e7\n        || std::isnan(this-&gt;nom_epsilon_))\n      return;\n\n    this-&gt;hamiltonian_.sample_p(this-&gt;z_, this-&gt;rand_int_);\n    this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n\n    // Guaranteed to be finite if randomly initialized\n    double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n\n    this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,\n                             logger);\n\n    double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n    if (std::isnan(h))\n      h = std::numeric_limits&lt;double&gt;::infinity();\n\n    double delta_H = H0 - h;\n\n    int direction = delta_H &gt; std::log(0.8) ? 1 : -1;\n\n    while (1) {\n      this-&gt;z_.ps_point::operator=(z_init);\n\n      this-&gt;hamiltonian_.sample_p(this-&gt;z_, this-&gt;rand_int_);\n      this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n\n      double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n\n      this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,\n                               logger);\n\n      double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n      if (std::isnan(h))\n        h = std::numeric_limits&lt;double&gt;::infinity();\n\n      double delta_H = H0 - h;\n\n      if ((direction == 1) && !(delta_H &gt; std::log(0.8)))\n        break;\n      else if ((direction == -1) && !(delta_H &lt; std::log(0.8)))\n        break;\n      else\n        this-&gt;nom_epsilon_ = direction == 1 ? 2.0 * this-&gt;nom_epsilon_\n                                            : 0.5 * this-&gt;nom_epsilon_;\n\n      if (this-&gt;nom_epsilon_ &gt; 1e7)\n        throw std::runtime_error(\n            \"Posterior is improper. \"\n            \"Please check your model.\");\n      if (this-&gt;nom_epsilon_ == 0)\n        throw std::runtime_error(\n            \"No acceptably small step size could \"\n            \"be found. Perhaps the posterior is \"\n            \"not continuous?\");\n    }\n\n    this-&gt;z_.ps_point::operator=(z_init);\n  }\nOn the lines\n    double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n    this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,logger);\n    double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n    double delta_H = H0 - h;\nthe Hamiltonian is first computed at the current point z_, then the integrator evolves the state, after which the Hamiltonian is computed at the new state. The change in Hamiltonian (delta_H) then determines how the nominal stepsize (nom_epsilon_) is refined, or if it is suitable so that the actual MCMC sampling can start."
  },
  {
    "objectID": "posts/post-02/index.html#base_nuts",
    "href": "posts/post-02/index.html#base_nuts",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.3 base_nuts",
    "text": "2.3 base_nuts\nThis is defined in base_nuts.hpp, and is the base class for No-U-Turn samplers with multinomial sampling. This class, which derives from base_hmc, is the first place where we encounter an implementation of transition(). You can read the comments in the source code of the transition() method to get and idea of the progress of the algorithm. We won’t go into details of the NUTS algorithm here, but just notice that the transition() function takes as input a sample& init_sample and generates a new sample object and returns it. Here, sample is a class that describes one MCMC draw, and it is defined in sample.hpp."
  },
  {
    "objectID": "posts/post-02/index.html#diag_e_nuts",
    "href": "posts/post-02/index.html#diag_e_nuts",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.4 diag_e_nuts",
    "text": "2.4 diag_e_nuts\nThis is a base class for NUTS with the diagonal HMC metric (diagonal mass matrix), and derives from base_nuts. There are similar classes for the dense and unit metric too. There are no new methods here."
  },
  {
    "objectID": "posts/post-02/index.html#adapt_diag_e_nuts",
    "href": "posts/post-02/index.html#adapt_diag_e_nuts",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.5 adapt_diag_e_nuts",
    "text": "2.5 adapt_diag_e_nuts\nFinally we get to the class the defines the adaptive NUTS sampler with the diagonal metric. This class, adapt_diag_e_nuts, derives from diag_e_nuts. It overrides the transition() method with\nsample transition(sample& init_sample, callbacks::logger& logger) {\n    sample s = diag_e_nuts&lt;Model, BaseRNG&gt;::transition(init_sample, logger);\n\n    if (this-&gt;adapt_flag_) {\n      this-&gt;stepsize_adaptation_.learn_stepsize(this-&gt;nom_epsilon_,\n                                                s.accept_stat());\n\n      bool update = this-&gt;var_adaptation_.learn_variance(this-&gt;z_.inv_e_metric_,\n                                                         this-&gt;z_.q);\n\n      if (update) {\n        this-&gt;init_stepsize(logger);\n\n        this-&gt;stepsize_adaptation_.set_mu(log(10 * this-&gt;nom_epsilon_));\n        this-&gt;stepsize_adaptation_.restart();\n      }\n    }\n    return s;\n  }\nThe actual transition\nsample s = diag_e_nuts&lt;Model, BaseRNG&gt;::transition(init_sample, logger);\nis still performed by calling the implementation of the parent class. This is the implementation defined in base_nuts, because diag_e_nuts does not override it. The other code is for adapting the HMC metric and the integrator step size. To study how this work, we need to find out what stepsize_adaptation_ and var_adaptation_ are. However, we can’t seem to find them in adapt_diag_e_nuts.hpp. So what is again going on?"
  },
  {
    "objectID": "posts/post-02/index.html#class-hierarchy",
    "href": "posts/post-02/index.html#class-hierarchy",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "3.1 Class hierarchy",
    "text": "3.1 Class hierarchy\n\n\n\nThe above diagram explains why sampler objects of class adapt_diag_e_nuts have the stepsize_adaptation_ and var_adaptation_ attributes. The former is for adapting the integrator step size and the latter for adapting the mass matrix diagonal. The metric is adapted in three windows, which are called init_buffer, term_buffer and base_window. The the most abstract base class is base_adaptation, which doesn’t contain any implemented methods."
  },
  {
    "objectID": "posts/post-02/index.html#algorithms",
    "href": "posts/post-02/index.html#algorithms",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "3.2 Algorithms",
    "text": "3.2 Algorithms\nLet’s now return to studying the transition() method of the adapt_diag_e_nuts class. When the adapt_flag_ is on (that is, in the warmup phase), each transition will run the code\n  this-&gt;stepsize_adaptation_.learn_stepsize(this-&gt;nom_epsilon_,\n                                                s.accept_stat());\n\n  bool update = this-&gt;var_adaptation_.learn_variance(this-&gt;z_.inv_e_metric_,\n                                                         this-&gt;z_.q);\n\n  if (update) {\n    this-&gt;init_stepsize(logger);\n\n    this-&gt;stepsize_adaptation_.set_mu(log(10 * this-&gt;nom_epsilon_));\n    this-&gt;stepsize_adaptation_.restart();\n  }\n\n3.2.1 learn_stepsize()\nThe learn_stepsize() algorithm is defined in stepsize_adaptation.hpp.\nvoid learn_stepsize(double& epsilon, double adapt_stat) {\n    ++counter_;\n\n    adapt_stat = adapt_stat &gt; 1 ? 1 : adapt_stat;\n\n    // Nesterov Dual-Averaging of log(epsilon)\n    const double eta = 1.0 / (counter_ + t0_);\n\n    s_bar_ = (1.0 - eta) * s_bar_ + eta * (delta_ - adapt_stat);\n\n    const double x = mu_ - s_bar_ * std::sqrt(counter_) / gamma_;\n    const double x_eta = std::pow(counter_, -kappa_);\n\n    x_bar_ = (1.0 - x_eta) * x_bar_ + x_eta * x;\n\n    epsilon = std::exp(x);\n  }\n\n\n3.2.2 learn_variance()\nThe learn_variance() algorithm on the other hand is defined in var_adaptation.hpp\nbool learn_variance(Eigen::VectorXd& var, const Eigen::VectorXd& q) {\n    if (adaptation_window())\n      estimator_.add_sample(q);\n\n    if (end_adaptation_window()) {\n      compute_next_window();\n\n      estimator_.sample_variance(var);\n\n      double n = static_cast&lt;double&gt;(estimator_.num_samples());\n      var = (n / (n + 5.0)) * var\n            + 1e-3 * (5.0 / (n + 5.0)) * Eigen::VectorXd::Ones(var.size());\n\n      if (!var.allFinite())\n        throw std::runtime_error(...);\n\n      estimator_.restart();\n\n      ++adapt_window_counter_;\n      return true;\n    }\n\n    ++adapt_window_counter_;\n    return false;\n  }\nThe interesting thing about this is that it returns true at the end of the three adaptation windows, and otherwise false. This return value (update) determines what happens at the end of the transition.\n\n\n3.2.3 update\n  if (update) {\n    this-&gt;init_stepsize(logger);\n\n    this-&gt;stepsize_adaptation_.set_mu(log(10 * this-&gt;nom_epsilon_));\n    this-&gt;stepsize_adaptation_.restart();\n  }\nThis part is again an interesting algorithmic detail of the adaptation. We see that at the end of each window of metric adaptation, the stepsize adaptation is restarted from 10 times the average stepsize from the previous window. See this thread for some motivation, discussion, and potential problems caused by this approach."
  },
  {
    "objectID": "posts/post-03/index.html",
    "href": "posts/post-03/index.html",
    "title": "The odemodeling R package",
    "section": "",
    "text": "The odemodeling R package is meant for Bayesian inference of ODE models in Stan. It is a bit clumsy to define a model in it, but once it is done, you can easily change between different ODE solvers, visualize fitted models. You can also study whether your ODE solver is reliable in your application. This is why you can try to use coarse solvers without error control (or control with high tolerances), and see if they are potentially faster than the standard ODE solvers in Stan with default tolerances.\n\nlibrary(odemodeling)\n#&gt; Attached odemodeling 0.2.3.\nlibrary(ggplot2)\n\n\n1 Creating a model\nAll models need to involve an ODE system of the form \\[\\begin{equation}\n    \\label{eq: ode}\n    \\frac{\\text{d} \\textbf{y}(t)}{\\text{d}t} = f_{\\psi}\\left(\\textbf{y}(t), t\\right),\n\\end{equation}\\] where \\(f_{\\psi}: \\mathbb{R}^D \\rightarrow \\mathbb{R}^D\\) with parameters \\(\\psi\\). As an example we define an ODE system \\[\\begin{equation}\n    \\label{eq: sho}\n    f_{\\psi}\\left(\\textbf{y}, t\\right) =\n    \\begin{bmatrix}\n      y_2 \\\\\n      - y_1 - \\theta y_2\n    \\end{bmatrix}\n\\end{equation}\\] describing a simple harmonic oscillator, where \\(\\psi = \\{ k \\}\\) and dimension \\(D = 2\\). The Stan code for the body of this function is\n\nsho_fun_body &lt;- \"\n  vector[2] dy_dt;\n  dy_dt[1] = y[2];\n  dy_dt[2] = - y[1] - k*y[2];\n  return(dy_dt);\n\"\n\nWe need to define the variable for the initial system state at t0 as y0. The ODE system dimension is declared as D and number of time points as N.\n\nN &lt;- stan_dim(\"N\", lower = 1)\nD &lt;- stan_dim(\"D\")\ny0 &lt;- stan_vector(\"y0\", length = D)\nk &lt;- stan_param(stan_var(\"k\", lower = 0), \"inv_gamma(5, 1)\")\n\nFinally we declare the parameter k and its prior.\n\nk &lt;- stan_param(stan_var(\"k\", lower = 0), prior = \"inv_gamma(5, 1)\")\n\nThe following code creates and compiles the Stan model.\n\nsho &lt;- ode_model(N,\n  odefun_vars = list(k),\n  odefun_body = sho_fun_body,\n  odefun_init = y0\n)\nprint(sho)\n#&gt; An object of class OdeModel. \n#&gt;  * ODE dimension: int D;\n#&gt;  * Time points array dimension: int&lt;lower=1&gt; N;\n#&gt;  * Number of significant figures in csv files: 18\n#&gt;  * Has likelihood: FALSE\n\nAs we see, all variables that affect the function \\(f_{\\psi}\\) need to be given as the odefun_vars argument. The function body itself is then the odefun_body argument. In this function body, we can use the following variables without having to declare them or write Stan code that computes them:\n\nThe ODE state y, which is a vector of same length as dimension of y0.\nAny variables that we give as odefun_vars for ode_model.\nAny variables that are dimensions of odefun_vars.\n\nThe initial state needs to be given as odefun_init. See the documentation of the ode_model function for more information.\nWe could call print(sho$stanmodel) to see the entire generated Stan model code.\n\n\n2 Sampling from prior\nWe can sample from the prior distribution of model parameters like so.\n\nsho_fit_prior &lt;- sho$sample(\n  t0 = 0.0,\n  t = seq(0.1, 10, by = 0.1),\n  data = list(y0 = c(1, 0), D = 2),\n  refresh = 0,\n  solver = rk45(\n    abs_tol = 1e-13,\n    rel_tol = 1e-13,\n    max_num_steps = 1e9\n  )\n)\n#&gt; Running MCMC with 4 sequential chains...\n#&gt; \n#&gt; Chain 1 finished in 0.4 seconds.\n#&gt; Chain 2 finished in 0.4 seconds.\n#&gt; Chain 3 finished in 0.4 seconds.\n#&gt; Chain 4 finished in 0.4 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.4 seconds.\n#&gt; Total execution time: 2.1 seconds.\n\nWe can view a summary of results\n\nprint(sho_fit_prior)\n#&gt; An object of class OdeModelMCMC. \n#&gt;  * Number of chains: 4\n#&gt;  * Number of iterations: 1000\n#&gt;  * Total time: 2.125 seconds.\n#&gt;  * Used solver: rk45(abs_tol=1e-13, rel_tol=1e-13, max_num_steps=1e+09)\n\nWe can obtain the ODE solution using each parameter draw by doing\n\nys &lt;- sho_fit_prior$extract_odesol_df()\n\nWe can plot ODE solutions like so\n\nsho_fit_prior$plot_odesol(alpha = 0.3)\n#&gt; Randomly selecting a subset of 100 draws to plot. Set draw_inds=0 to plot all 4000 draws.\n\n\n\n\n\n\n\n\nWe can plot the distribution of ODE solutions like so\n\nsho_fit_prior$plot_odesol_dist(include_y0 = TRUE)\n#&gt; plotting medians and 80% central intervals\n\n\n\n\n\n\n\n\nWe can plot ODE solution using one draw like so\n\nsho_fit_prior$plot_odesol(draw_inds = 45)\n\n\n\n\n\n\n\n\n\n\n3 Using different ODE solvers\nWe generate quantities using a different solver and different output time points t. Possible solvers are rk45(),bdf(), adams(), ckrk(), midpoint(), and rk4(). Of these the first four are adaptive and built-in to Stan, where as the last two take a fixed number of steps and are written in Stan code.\n\ngq_bdf &lt;- sho_fit_prior$gqs(solver = bdf(tol = 1e-4), t = seq(0.5, 10, by = 0.5))\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n\n\ngq_mp &lt;- sho_fit_prior$gqs(solver = midpoint(num_steps = 4), t = seq(0.5, 10, by = 0.5))\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n\nWe can again plot ODE solution using one draw like so\n\ngq_bdf$plot_odesol(draw_inds = 45)\n\n\n\n\n\n\n\ngq_mp$plot_odesol(draw_inds = 45)\n\n\n\n\n\n\n\n\n\n\n4 Defining a likelihood\nNext we assume that we have some data vector y1_obs and define a likelihood function.\n\nsho_loglik_body &lt;- \"\n  real loglik = 0.0;\n  for(n in 1:N) {\n    loglik += normal_lpdf(y1_obs[n] | y_sol[n][1], sigma);\n  }\n  return(loglik);\n\"\n\nIn this function body, we can use the following variables without having to declare them or write Stan code that computes them:\n\nThe ODE solution y_sol.\nAny variables that we give as loglik_vars for ode_model.\nAny variables that are dimensions of loglik_vars.\n\nHere we define as loglik_vars a sigma which is a noise magnitude parameter, and the data y1_obs. Notice that we can also use the N variable in loglik_body, because it is a dimension (length) of y1_obs.\n\nsigma &lt;- stan_param(stan_var(\"sigma\", lower = 0), prior = \"normal(0, 2)\")\ny1_obs &lt;- stan_vector(\"y1_obs\", length = N)\n\nThe following code creates and compiles the posterior Stan model.\n\nsho_post &lt;- ode_model(N,\n  odefun_vars = list(k),\n  odefun_body = sho_fun_body,\n  odefun_init = y0,\n  loglik_vars = list(sigma, y1_obs),\n  loglik_body = sho_loglik_body\n)\nprint(sho_post)\n#&gt; An object of class OdeModel. \n#&gt;  * ODE dimension: int D;\n#&gt;  * Time points array dimension: int&lt;lower=1&gt; N;\n#&gt;  * Number of significant figures in csv files: 18\n#&gt;  * Has likelihood: TRUE\n\n\n\n5 Sampling from posterior\nNow if we have some data\n\ny1_obs &lt;- c(\n  0.801, 0.391, 0.321, -0.826, -0.234, -0.663, -0.756, -0.717,\n  -0.078, -0.083, 0.988, 0.878, 0.300, 0.307, 0.270, -0.464, -0.403,\n  -0.295, -0.186, 0.158\n)\nt_obs &lt;- seq(0.5, 10, by = 0.5)\n\nand assume that initial state y0 = c(1, 0) is known, we can fit the model\n\nsho_fit_post &lt;- sho_post$sample(\n  t0 = 0.0,\n  t = t_obs,\n  data = list(y0 = c(1, 0), D = 2, y1_obs = y1_obs),\n  refresh = 0,\n  solver = midpoint(2)\n)\n#&gt; Running MCMC with 4 sequential chains...\n#&gt; Chain 1 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n#&gt; Chain 1 Exception: Exception: normal_lpdf: Location parameter is nan, but must be finite! (in '/var/folders/c6/f7ggr5956fs82hyz_p2qzsp00000gn/T/RtmpHS3GCA/model-152962319b414.stan', line 139, column 6 to column 60) (in '/var/folders/c6/f7ggr5956fs82hyz_p2qzsp00000gn/T/RtmpHS3GCA/model-152962319b414.stan', line 166, column 2 to column 62)\n#&gt; Chain 1 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n#&gt; Chain 1 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n#&gt; Chain 1\n#&gt; Chain 1 finished in 0.2 seconds.\n#&gt; Chain 2 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n#&gt; Chain 2 Exception: Exception: normal_lpdf: Location parameter is nan, but must be finite! (in '/var/folders/c6/f7ggr5956fs82hyz_p2qzsp00000gn/T/RtmpHS3GCA/model-152962319b414.stan', line 139, column 6 to column 60) (in '/var/folders/c6/f7ggr5956fs82hyz_p2qzsp00000gn/T/RtmpHS3GCA/model-152962319b414.stan', line 166, column 2 to column 62)\n#&gt; Chain 2 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n#&gt; Chain 2 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n#&gt; Chain 2\n#&gt; Chain 2 finished in 0.2 seconds.\n#&gt; Chain 3 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n#&gt; Chain 3 Exception: Exception: normal_lpdf: Location parameter is nan, but must be finite! (in '/var/folders/c6/f7ggr5956fs82hyz_p2qzsp00000gn/T/RtmpHS3GCA/model-152962319b414.stan', line 139, column 6 to column 60) (in '/var/folders/c6/f7ggr5956fs82hyz_p2qzsp00000gn/T/RtmpHS3GCA/model-152962319b414.stan', line 166, column 2 to column 62)\n#&gt; Chain 3 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n#&gt; Chain 3 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n#&gt; Chain 3\n#&gt; Chain 3 finished in 0.2 seconds.\n#&gt; Chain 4 Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n#&gt; Chain 4 Exception: Exception: normal_lpdf: Location parameter is -inf, but must be finite! (in '/var/folders/c6/f7ggr5956fs82hyz_p2qzsp00000gn/T/RtmpHS3GCA/model-152962319b414.stan', line 139, column 6 to column 60) (in '/var/folders/c6/f7ggr5956fs82hyz_p2qzsp00000gn/T/RtmpHS3GCA/model-152962319b414.stan', line 166, column 2 to column 62)\n#&gt; Chain 4 If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n#&gt; Chain 4 but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n#&gt; Chain 4\n#&gt; Chain 4 finished in 0.2 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.2 seconds.\n#&gt; Total execution time: 1.3 seconds.\n\nWe fit the posterior distribution of ODE solutions against the data\n\nplt &lt;- sho_fit_post$plot_odesol_dist()\n#&gt; plotting medians and 80% central intervals\ndf_data &lt;- data.frame(t_obs, y1_obs, ydim = rep(\"y1\", length(t_obs)))\ncolnames(df_data) &lt;- c(\"t\", \"y\", \"ydim\")\ndf_data$ydim &lt;- as.factor(df_data$ydim)\nplt &lt;- plt + geom_point(data = df_data, aes(x = t, y = y), inherit.aes = FALSE)\nplt\n\n\n\n\n\n\n\n\n\n\n6 Reliability of ODE solver\nFinally we can study whether the solver we used during MCMC (midpoint(2)) was accurate enough. This is done by solving the system using increasingly more numbers of steps in the solver, and studying different metrics computed using the ODE solutions and corresponding likelihood values.\n\nsolvers &lt;- midpoint_list(c(4, 6, 8, 10, 12, 14, 16, 18))\nrel &lt;- sho_fit_post$reliability(solvers = solvers)\n#&gt; directory 'results' doesn't exist, creating it\n#&gt; Running GQ using MCMC-time configuration.\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n#&gt; ==============================================================\n#&gt;  (1) Running GQ with: midpoint(num_steps=4)\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n#&gt; Saving result object to results/odegq_1.rds\n#&gt; ==============================================================\n#&gt;  (2) Running GQ with: midpoint(num_steps=6)\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n#&gt; Saving result object to results/odegq_2.rds\n#&gt; ==============================================================\n#&gt;  (3) Running GQ with: midpoint(num_steps=8)\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n#&gt; Saving result object to results/odegq_3.rds\n#&gt; ==============================================================\n#&gt;  (4) Running GQ with: midpoint(num_steps=10)\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n#&gt; Saving result object to results/odegq_4.rds\n#&gt; ==============================================================\n#&gt;  (5) Running GQ with: midpoint(num_steps=12)\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n#&gt; Saving result object to results/odegq_5.rds\n#&gt; ==============================================================\n#&gt;  (6) Running GQ with: midpoint(num_steps=14)\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n#&gt; Saving result object to results/odegq_6.rds\n#&gt; ==============================================================\n#&gt;  (7) Running GQ with: midpoint(num_steps=16)\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n#&gt; Saving result object to results/odegq_7.rds\n#&gt; ==============================================================\n#&gt;  (8) Running GQ with: midpoint(num_steps=18)\n#&gt; Running standalone generated quantities after 4 MCMC chains, 1 chain at a time ...\n#&gt; \n#&gt; Chain 1 finished in 0.0 seconds.\n#&gt; Chain 2 finished in 0.0 seconds.\n#&gt; Chain 3 finished in 0.0 seconds.\n#&gt; Chain 4 finished in 0.0 seconds.\n#&gt; \n#&gt; All 4 chains finished successfully.\n#&gt; Mean chain execution time: 0.0 seconds.\n#&gt; Total execution time: 0.5 seconds.\n#&gt; Saving result object to results/odegq_8.rds\nprint(rel$metrics)\n#&gt;     pareto_k    n_eff     r_eff mad_loglik mad_odesol\n#&gt; 1 0.04886421 2575.384 0.6572188   1.230924 0.05165610\n#&gt; 2 0.06202465 2545.578 0.6561548   1.478181 0.06111844\n#&gt; 3 0.07737437 2533.872 0.6558465   1.566171 0.06441659\n#&gt; 4 0.07305680 2528.211 0.6557333   1.607148 0.06593929\n#&gt; 5 0.07093196 2525.093 0.6556848   1.629471 0.06676499\n#&gt; 6 0.06961408 2523.206 0.6556619   1.642952 0.06726222\n#&gt; 7 0.06871202 2521.980 0.6556503   1.651710 0.06758463\n#&gt; 8 0.06808765 2521.140 0.6556442   1.657718 0.06780550\nunlink(\"results\")\n\nThe mad_odesol and mad_loglik are the maximum absolute difference in the ODE solutions and log likelihood, respectively, over all MCMC draws. The former is denoted MAE in Timonen et. al (2023). Please refer to that paper in order to interpret the pareto_k column. Briefly we note that the Pareto-k values seem to be converging to a value smaller than 0.5, meaning that importance sampling is possible and we don’t need to run MCMC again.\n\n\n7 References\n\nTimonen, J., Siccha, N., Bales, B., Lähdesmäki, H., & Vehtari, A. (2023). An importance sampling approach for reliable and efficient inference in Bayesian ordinary differential equation models. Stat, 12(1), e614. link\n\n\n\n\n\nReuseCC BY 4.0"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Hobby projects",
    "section": "",
    "text": "Simple programming projects."
  },
  {
    "objectID": "projects.html#snake-game",
    "href": "projects.html#snake-game",
    "title": "Hobby projects",
    "section": "Snake game",
    "text": "Snake game\nA simple game using raw Javascript.\n\nPlay\nSource"
  },
  {
    "objectID": "projects.html#darts-simulator",
    "href": "projects.html#darts-simulator",
    "title": "Hobby projects",
    "section": "Darts simulator",
    "text": "Darts simulator\nWhere should I aim given my skill level?\n\nSource"
  },
  {
    "objectID": "projects.html#pace-converter",
    "href": "projects.html#pace-converter",
    "title": "Hobby projects",
    "section": "Pace converter",
    "text": "Pace converter\nUseful for runners.\n\nTry online\nSource"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "I have written several R packages for probabilistic modeling that use Stan under the hood for fitting the models.\n\nodemodeling: Building and fitting Bayesian ODE models\nlgpr: Modeling longitudinal data using Gaussian processes\nlgpr2: Scalable Gaussian process modeling of longitudinal data"
  }
]