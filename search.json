[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Juho Timonen",
    "section": "",
    "text": "I am a doctoral student in the Computational Systems Biology group of Aalto University, Department of Computer Science. My research topics and interests include Gaussian processes for longitudinal data and Bayesian ordinary differential equation models. I work with Generable Inc., employing statistical models to support drug development."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Posts",
    "section": "",
    "text": "Blog posts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Stan codebase - Part 2: Samplers\n\n\n\n\n\n\nStan, C++\n\n\n\nStudying the C++ code of the NUTS algorithm of Stan.\n\n\n\n\n\nJan 14, 2022\n\n\nJuho Timonen\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Stan codebase - Part 1: Finding an entry point\n\n\n\n\n\n\nStan, C++\n\n\n\nOverview of the different libraries related to Stan and their organization, and finding and entry point to the internal C++ code.\n\n\n\n\n\nNov 29, 2021\n\n\nJuho Timonen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Scalable mixed-domain Gaussian process modeling and model reduction for longitudinal data. Timonen, J. & Lähdesmäki, H., 2024, arXiv\nAn importance sampling approach for reliable and efficient inference in Bayesian ordinary differential equation models. Timonen, J., Siccha, N., Bales, B., Lähdesmäki, H. & Vehtari, A., 2023, Stat. 12, 1, e614.\nlgpr: an interpretable non-parametric method for inferring covariate effects from longitudinal data. Timonen, J., Mannerström, H., Vehtari, A. & Lähdesmäki, H., 2021, Bioinformatics. 37, 13, p. 1860-1867.\nA probabilistic framework for molecular network structure inference by means of mechanistic modeling. Timonen, J., Mannerström, H., Lähdesmäki, H. & Intosalmi, J., 2018, IEEE-ACM Transactions on Computational Biology and Bioinformatics. 16, 6, p. 1843-1854"
  },
  {
    "objectID": "posts/post-01/index.html",
    "href": "posts/post-01/index.html",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "",
    "text": "So, you have your Stan model written and are doing inference for it, but something weird is happening? Or maybe you want to extend Stan but don’t know where to start because the source code repositories look daunting. These are some of the possible reasons why someone might want to study the internals of Stan, and what is happening under the hood. I have for various reasons for a long time wanted to just see what is happening line-by-line. In this post, I am going to look at how a typical program execution starts to travel through all the different libraries related to Stan, using CmdStanR as the starting point.\n\nRelationships between different libraries and various interfaces related to Stan are visualized in the above diagram. The C++ core that we study in this post is organized into three parts.\n\nCmdStan: A command-line interface to Stan\nStan: The MCMC and optimization algorithms\nStan Math: Mathematical functions and their gradients (automatic differentiation)\n\nMany higher-level interfaces, like CmdStanR and CmdStanPy, call CmdStan internally. RStan and PyStan employ different strategies that do not rely on CmdStan. A benefit of CmdStan is that it is always released simultaneously with Stan with the same version number, which means that CmdStan is always up to date. In this, post we study the most recent Stan version 2.28.2, and if the source code structure doesn’t experience dramatic changes in the near future, this post might stay relevant for future versions too."
  },
  {
    "objectID": "posts/post-01/index.html#creating-the-executable",
    "href": "posts/post-01/index.html#creating-the-executable",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "2.1 Creating the executable",
    "text": "2.1 Creating the executable\nThe first thing we look at is cmdstan_model(stan_file = \"mymodel.stan\"). This does two interesting things.\n\nTranspiles the Stan model to C++ code using stanc3\nCompiles the C++ code into an executable file mymodel.exe (without the .exe file suffix on Mac or Linux).\n\nWe could have used model$save_hpp_file() to save the model C++ code into mymodel.hpp if we wanted to look at that. However, we are now interested in the C++ code that doesn’t depend on the model. I would imagine that also a lot of this model-independent code has to go into the executable."
  },
  {
    "objectID": "posts/post-01/index.html#running-the-executable",
    "href": "posts/post-01/index.html#running-the-executable",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "2.2 Running the executable",
    "text": "2.2 Running the executable\nThe call model$sample(adapt_delta = 0.95, refresh = 100) creates four processes (because the default number of chains is four) that each run the executable. For example, the first process creates the command-line call\nmymodel.exe id=1 random seed=660816326 output file=&lt;opath&gt;.csv refresh=100 profile_file=&lt;ppath&gt;.csv method=sample save_warmup=0 algorithm=hmc engine=nuts adapt delta=0.95 engaged=1\nwhere &lt;opath&gt; and &lt;ppath&gt; are paths to some temporary CSV files on the computer. Arguments delta=0.95 and refresh=100 are things that we specified and others are defaults created by CmdStanR. You can find explanations for the command-line arguments in the CmdStan User’s Guide.\nFor other processes the id argument is 2, 3, and 4. From now on we study only one chain (the one with id=1) and next try to find the entry point in the CmdStan code that is started with the above command-line instruction."
  },
  {
    "objectID": "posts/post-01/index.html#main.cpp",
    "href": "posts/post-01/index.html#main.cpp",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "3.1 main.cpp",
    "text": "3.1 main.cpp\nWe find a main.cpp, which looks promising. It actually includes an\nint main(int argc, const char *argv[]) {\n  // ...\n}\nfunction which is the starting point of any C++ program. Based on our command line arguments, at this point argc (number of command line arguments) should be 15, argv[0] should be \"mymodel.exe\", argv[1] should be \"id=1\" and so on. We see that main just calls cmdstan::command(argc, argv), which is defined in command.hpp."
  },
  {
    "objectID": "posts/post-01/index.html#command.hpp",
    "href": "posts/post-01/index.html#command.hpp",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "3.2 command.hpp",
    "text": "3.2 command.hpp\nInside the command() function is a huge if-else parade which is quite difficult to read. So here is a high-level summary of the control flow inside it.\nint command(int argc, const char *argv[]) {\n\n  // ... parse the arguments\n  // ... initialize model\n  // ... initialize writers\n\n  if (user_method-&gt;arg(\"generate_quantities\")) {\n    // ...\n  } else if (user_method-&gt;arg(\"diagnose\")) {\n    // ...\n  } else if (user_method-&gt;arg(\"optimize\")) {\n    // ...\n  } else if (user_method-&gt;arg(\"sample\")) {\n    // ...\n    if (model.num_params_r() == 0 || algo-&gt;value() == \"fixed_param\") {\n      // ...\n    } else if (algo-&gt;value() == \"hmc\") {\n      // ...\n      if (adapt_engaged == true && num_warmup == 0) {\n        // ... error\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == false && metric_supplied == false) {\n        // ... \n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == false && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == true && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == true && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == false && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == false && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == true && metric_supplied == false) {\n        // ... WE END UP HERE\n        return_code = stan::services::sample::hmc_nuts_diag_e_adapt(\n            model, num_chains, init_contexts, random_seed, id, init_radius,\n            num_warmup, num_samples, num_thin, save_warmup, refresh, stepsize,\n            stepsize_jitter, max_depth, delta, gamma, kappa, t0, init_buffer,\n            term_buffer, window, interrupt, logger, init_writers,\n            sample_writers, diagnostic_writers);\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == true && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"unit_e\"\n                 && adapt_engaged == false) {\n        // ...\n      } else if (engine-&gt;value() == \"nuts\" && metric-&gt;value() == \"unit_e\"\n                 && adapt_engaged == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == false && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == false && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == true && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"dense_e\"\n                 && adapt_engaged == true && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == false && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == false && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == true && metric_supplied == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"diag_e\"\n                 && adapt_engaged == true && metric_supplied == true) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"unit_e\"\n                 && adapt_engaged == false) {\n        // ...\n      } else if (engine-&gt;value() == \"static\" && metric-&gt;value() == \"unit_e\"\n                 && adapt_engaged == true) {\n        // ...\n      }\n    }\n  } else if (user_method-&gt;arg(\"variational\")) {\n    // ...\n  }\n  // ...\n  return return_code;\n}\nIn most of the branches, the left-out part // ... ends up calling something from stan::services. This is also the case in our example, and because our method argument is sample,the default algorithm is NUTS with adaptation engaged and the default metric is diagonal (and we haven’t supplied the metric), we will call stan::services::sample::hmc_nuts_diag_e_adapt(). We will therefore now jump from CmdStan to Stan. Hooray!"
  },
  {
    "objectID": "posts/post-01/index.html#hmc_nuts_diag_e_adapt.hpp",
    "href": "posts/post-01/index.html#hmc_nuts_diag_e_adapt.hpp",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "4.1 hmc_nuts_diag_e_adapt.hpp",
    "text": "4.1 hmc_nuts_diag_e_adapt.hpp\nIn services/sample we find hmc_nuts_diag_e_adapt.hpp which contains the function that we called from CmdStan. But wait, it is actually overloaded with four different versions of it with the same name. Not to mention each of these are templated. We will not stop here to think much about why these all versions of hmc_nuts_diag_e_adapt() are needed. A very valid print debugging approach reveals that in our case, we call the fourth one, which calls the second one, which then calls the first one. There we have\nstd::vector&lt;double&gt; cont_vector = util::initialize(\n      model, init, rng, init_radius, true, logger, init_writer);\nwhere the parameter values are initialized. In initialize(), which is defined in initialize.hpp, we try most 100 random initial points, until a point where log probability and its gradient can be evaluated successfully. In our case the first try is successful. Therefore we can now think that we exist somewhere in the (unconstrained) parameter space, at a point stored in cont_vector. The next step is to call\n  util::run_adaptive_sampler(\n      sampler, model, cont_vector, num_warmup, num_samples, num_thin, refresh,\n      save_warmup, rng, interrupt, logger, sample_writer, diagnostic_writer);\nwhich we will look at next."
  },
  {
    "objectID": "posts/post-01/index.html#run_adaptive_sampler.hpp",
    "href": "posts/post-01/index.html#run_adaptive_sampler.hpp",
    "title": "Understanding the Stan codebase - Part 1: Finding an entry point",
    "section": "4.2 run_adaptive_sampler.hpp",
    "text": "4.2 run_adaptive_sampler.hpp\nSo we are now at run_adaptive_sampler.hpp which is in services/util. There we have three interesting parts.\n\n\nInitializing stepsize\n\n\nGenerating transitions, adaptation engaged (warmup)\n\n\nGenerating transitions, adaptation disengaged (sampling)\n\n\nThe part\nsampler.init_stepsize(logger)\ninitializes the stepsize and is defined in mcmc/hmc/base_hmc.hpp. This already involves a bit of Hamiltonian computations and evolving the Leapfrog integrator. After this, we start to actually generate MCMC transitions using the sampler. This is done in two phases with calls to\n  util::generate_transitions();\nand in the first one we have adaptation engaged. We will look at generate_transitions() in the next blog post. Spoiler alert: sampler.init_stepsize() will be called there again so we will also look at it more."
  },
  {
    "objectID": "posts/post-02/index.html",
    "href": "posts/post-02/index.html",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "",
    "text": "We pick up from where we left off in Part 1. We found out that CmdStan calls the Stan services in cmdstan::command(). For example with the command-line call\nmymodel.exe id=1 method=sample algorithm=hmc engine=nuts adapt engaged=1\n\nthe called service is stan::services::sample::hmc_nuts_diag_adapt()\nwhich then calls stan::services::util::run_adaptive_sampler()\nwhich calls stan::services::util::generate_transitions().\n\nNote: All code pieces shown from now on in this post are adapted from the stan source code, licenced under the new BSD licence. Comments starting with ... indicate parts that have been left out from original source code. During writing of this post, the most recent Stan version is 2.28.2. The hyperlinks to source code cannot be guaranteed to work in the future, if the source repo organization is changed or files are renamed.\n\n\n\nWe find generate_transitions() in generate_transitions.hpp.\nvoid generate_transitions(stan::mcmc::base_mcmc& sampler, int num_iterations,\n                          ..., stan::mcmc::sample& init_s, Model& model, ...) {\n  for (int m = 0; m &lt; num_iterations; ++m) {\n    // ... callbacks and progress printing\n    init_s = sampler.transition(init_s, logger);\n    // ... writing to output\n  }\n}\nAmong other things, it takes as input the sampler, model, and initial point in parameter space. The function is basically just a loop that calls sampler.transition() repeatedly for num_iterations times. Therefore, all interesting algorithmic code and properties that define how sampling works, whether adaptation is performed etc, have to be included as part of sampler.\nThis is why we now jump from the stan::services namespace to stan::mcmc, where the different samplers and their transitions are defined."
  },
  {
    "objectID": "posts/post-02/index.html#recap-of-part-1",
    "href": "posts/post-02/index.html#recap-of-part-1",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "",
    "text": "We pick up from where we left off in Part 1. We found out that CmdStan calls the Stan services in cmdstan::command(). For example with the command-line call\nmymodel.exe id=1 method=sample algorithm=hmc engine=nuts adapt engaged=1\n\nthe called service is stan::services::sample::hmc_nuts_diag_adapt()\nwhich then calls stan::services::util::run_adaptive_sampler()\nwhich calls stan::services::util::generate_transitions().\n\nNote: All code pieces shown from now on in this post are adapted from the stan source code, licenced under the new BSD licence. Comments starting with ... indicate parts that have been left out from original source code. During writing of this post, the most recent Stan version is 2.28.2. The hyperlinks to source code cannot be guaranteed to work in the future, if the source repo organization is changed or files are renamed."
  },
  {
    "objectID": "posts/post-02/index.html#starting-point-for-part-2",
    "href": "posts/post-02/index.html#starting-point-for-part-2",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "",
    "text": "We find generate_transitions() in generate_transitions.hpp.\nvoid generate_transitions(stan::mcmc::base_mcmc& sampler, int num_iterations,\n                          ..., stan::mcmc::sample& init_s, Model& model, ...) {\n  for (int m = 0; m &lt; num_iterations; ++m) {\n    // ... callbacks and progress printing\n    init_s = sampler.transition(init_s, logger);\n    // ... writing to output\n  }\n}\nAmong other things, it takes as input the sampler, model, and initial point in parameter space. The function is basically just a loop that calls sampler.transition() repeatedly for num_iterations times. Therefore, all interesting algorithmic code and properties that define how sampling works, whether adaptation is performed etc, have to be included as part of sampler.\nThis is why we now jump from the stan::services namespace to stan::mcmc, where the different samplers and their transitions are defined."
  },
  {
    "objectID": "posts/post-02/index.html#base_mcmc",
    "href": "posts/post-02/index.html#base_mcmc",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.1 base_mcmc",
    "text": "2.1 base_mcmc\nThis is just an interface for all MCMC samplers, as it doesn’t contain any function bodies.\nclass base_mcmc {\n public:\n  base_mcmc() {}\n\n  virtual ~base_mcmc() {}\n\n  virtual sample transition(sample& init_sample, callbacks::logger& logger) = 0;\n\n  virtual void get_sampler_param_names(std::vector&lt;std::string&gt;& names) {}\n\n  virtual void get_sampler_params(std::vector&lt;double&gt;& values) {}\n\n  //... other virtual functions without body\n\n};\nThe class member functions are all virtual (except the constructor), meaning that deriving classes can override them. We see that transition() is pure virtual (declared with = 0), meaning that a deriving class must override it in order to be instantiable."
  },
  {
    "objectID": "posts/post-02/index.html#base_hmc",
    "href": "posts/post-02/index.html#base_hmc",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.2 base_hmc",
    "text": "2.2 base_hmc\nThis is a base for all Hamiltonian samplers, and derives from base_mcmc.\ntemplate &lt;class Model, template &lt;class, class&gt; class Hamiltonian,\n          template &lt;class&gt; class Integrator, class BaseRNG&gt;\nclass base_hmc : public base_mcmc {\n public:\n  base_hmc(const Model& model, BaseRNG& rng)\n      : base_mcmc(),\n        z_(model.num_params_r()),\n        integrator_(),\n        hamiltonian_(model),\n        rand_int_(rng),\n        rand_uniform_(rand_int_),\n        nom_epsilon_(0.1),\n        epsilon_(nom_epsilon_),\n        epsilon_jitter_(0.0) {}\n\n  // ...\n\n  void seed(const Eigen::VectorXd& q) { z_.q = q; }\n\n  void init_hamiltonian(callbacks::logger& logger) {\n    this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n  }\n\n  void init_stepsize(callbacks::logger& logger) {\n    ps_point z_init(this-&gt;z_);\n\n    // Skip initialization for extreme step sizes\n    if (this-&gt;nom_epsilon_ == 0 || this-&gt;nom_epsilon_ &gt; 1e7\n        || std::isnan(this-&gt;nom_epsilon_))\n      return;\n\n    this-&gt;hamiltonian_.sample_p(this-&gt;z_, this-&gt;rand_int_);\n    this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n\n    // Guaranteed to be finite if randomly initialized\n    double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n\n    this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,\n                             logger);\n\n    double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n    if (std::isnan(h))\n      h = std::numeric_limits&lt;double&gt;::infinity();\n\n    double delta_H = H0 - h;\n\n    int direction = delta_H &gt; std::log(0.8) ? 1 : -1;\n\n    while (1) {\n      this-&gt;z_.ps_point::operator=(z_init);\n\n      this-&gt;hamiltonian_.sample_p(this-&gt;z_, this-&gt;rand_int_);\n      this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n\n      double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n\n      this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,\n                               logger);\n\n      double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n      if (std::isnan(h))\n        h = std::numeric_limits&lt;double&gt;::infinity();\n\n      double delta_H = H0 - h;\n\n      if ((direction == 1) && !(delta_H &gt; std::log(0.8)))\n        break;\n      else if ((direction == -1) && !(delta_H &lt; std::log(0.8)))\n        break;\n      else\n        this-&gt;nom_epsilon_ = direction == 1 ? 2.0 * this-&gt;nom_epsilon_\n                                            : 0.5 * this-&gt;nom_epsilon_;\n\n      if (this-&gt;nom_epsilon_ &gt; 1e7)\n        throw std::runtime_error(\n            \"Posterior is improper. \"\n            \"Please check your model.\");\n      if (this-&gt;nom_epsilon_ == 0)\n        throw std::runtime_error(\n            \"No acceptably small step size could \"\n            \"be found. Perhaps the posterior is \"\n            \"not continuous?\");\n    }\n\n    this-&gt;z_.ps_point::operator=(z_init);\n  }\n\n  // ...\n\n  typename Hamiltonian&lt;Model, BaseRNG&gt;::PointType& z() { return z_; }\n\n  const typename Hamiltonian&lt;Model, BaseRNG&gt;::PointType& z() const noexcept {\n    return z_;\n  }\n\n  // ... setters and getters for the protected properties\n\n  void sample_stepsize() {\n    this-&gt;epsilon_ = this-&gt;nom_epsilon_;\n    if (this-&gt;epsilon_jitter_)\n      this-&gt;epsilon_\n          *= 1.0 + this-&gt;epsilon_jitter_ * (2.0 * this-&gt;rand_uniform_() - 1.0);\n  }\n\n protected:\n  typename Hamiltonian&lt;Model, BaseRNG&gt;::PointType z_;\n  Integrator&lt;Hamiltonian&lt;Model, BaseRNG&gt; &gt; integrator_;\n  Hamiltonian&lt;Model, BaseRNG&gt; hamiltonian_;\n\n  BaseRNG& rand_int_;\n\n  // Uniform(0, 1) RNG\n  boost::uniform_01&lt;BaseRNG&&gt; rand_uniform_;\n\n  double nom_epsilon_;\n  double epsilon_;\n  double epsilon_jitter_;\n};\nWe see that the class doesn’t implement transition(), so it is also an abstract class. What it does do is it defines some attributes that all Hamiltonian samplers need.\n\n2.2.1 class attributes\nThe attributes listed as protected describe the internal state of the sampler. All Hamiltonian samplers have these attributes, and the most interesting ones of them are\n\nz_: current state of the sampler (point in parameter space)\nintegrator_: numerical integrator used to simulate the Hamiltonian dynamics\nhamiltonian_: the Hamiltonian system\nepsilon_ / nom_epsilon_: step size of the integrator\n\n\n\n2.2.2 getters\nThe above are private attributes and should not be directly accessed from the outside. Instead, there are some getter methods that can be used, for example\n  typename Hamiltonian&lt;Model, BaseRNG&gt;::PointType& z() { return z_; }\nso one could use sampler.z() to get the current point in the (unconstrained) parameter space.\n\n\n2.2.3 init_stepsize()\nThe first actual algorithm that we encouter is the init_stepsize() method of the base_hmc class, defined in base_hmc.hpp. This defines how the integrator’s initial stepsize (possibly given by user) is refined. As we saw in Part 1, this gets called in stan::services::sample::hmc_nuts_diag_e_adapt() before any MCMC iterations are done.\nvoid init_stepsize(callbacks::logger& logger) {\n    ps_point z_init(this-&gt;z_);\n\n    // Skip initialization for extreme step sizes\n    if (this-&gt;nom_epsilon_ == 0 || this-&gt;nom_epsilon_ &gt; 1e7\n        || std::isnan(this-&gt;nom_epsilon_))\n      return;\n\n    this-&gt;hamiltonian_.sample_p(this-&gt;z_, this-&gt;rand_int_);\n    this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n\n    // Guaranteed to be finite if randomly initialized\n    double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n\n    this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,\n                             logger);\n\n    double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n    if (std::isnan(h))\n      h = std::numeric_limits&lt;double&gt;::infinity();\n\n    double delta_H = H0 - h;\n\n    int direction = delta_H &gt; std::log(0.8) ? 1 : -1;\n\n    while (1) {\n      this-&gt;z_.ps_point::operator=(z_init);\n\n      this-&gt;hamiltonian_.sample_p(this-&gt;z_, this-&gt;rand_int_);\n      this-&gt;hamiltonian_.init(this-&gt;z_, logger);\n\n      double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n\n      this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,\n                               logger);\n\n      double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n      if (std::isnan(h))\n        h = std::numeric_limits&lt;double&gt;::infinity();\n\n      double delta_H = H0 - h;\n\n      if ((direction == 1) && !(delta_H &gt; std::log(0.8)))\n        break;\n      else if ((direction == -1) && !(delta_H &lt; std::log(0.8)))\n        break;\n      else\n        this-&gt;nom_epsilon_ = direction == 1 ? 2.0 * this-&gt;nom_epsilon_\n                                            : 0.5 * this-&gt;nom_epsilon_;\n\n      if (this-&gt;nom_epsilon_ &gt; 1e7)\n        throw std::runtime_error(\n            \"Posterior is improper. \"\n            \"Please check your model.\");\n      if (this-&gt;nom_epsilon_ == 0)\n        throw std::runtime_error(\n            \"No acceptably small step size could \"\n            \"be found. Perhaps the posterior is \"\n            \"not continuous?\");\n    }\n\n    this-&gt;z_.ps_point::operator=(z_init);\n  }\nOn the lines\n    double H0 = this-&gt;hamiltonian_.H(this-&gt;z_);\n    this-&gt;integrator_.evolve(this-&gt;z_, this-&gt;hamiltonian_, this-&gt;nom_epsilon_,logger);\n    double h = this-&gt;hamiltonian_.H(this-&gt;z_);\n    double delta_H = H0 - h;\nthe Hamiltonian is first computed at the current point z_, then the integrator evolves the state, after which the Hamiltonian is computed at the new state. The change in Hamiltonian (delta_H) then determines how the nominal stepsize (nom_epsilon_) is refined, or if it is suitable so that the actual MCMC sampling can start."
  },
  {
    "objectID": "posts/post-02/index.html#base_nuts",
    "href": "posts/post-02/index.html#base_nuts",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.3 base_nuts",
    "text": "2.3 base_nuts\nThis is defined in base_nuts.hpp, and is the base class for No-U-Turn samplers with multinomial sampling. This class, which derives from base_hmc, is the first place where we encounter an implementation of transition(). You can read the comments in the source code of the transition() method to get and idea of the progress of the algorithm. We won’t go into details of the NUTS algorithm here, but just notice that the transition() function takes as input a sample& init_sample and generates a new sample object and returns it. Here, sample is a class that describes one MCMC draw, and it is defined in sample.hpp."
  },
  {
    "objectID": "posts/post-02/index.html#diag_e_nuts",
    "href": "posts/post-02/index.html#diag_e_nuts",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.4 diag_e_nuts",
    "text": "2.4 diag_e_nuts\nThis is a base class for NUTS with the diagonal HMC metric (diagonal mass matrix), and derives from base_nuts. There are similar classes for the dense and unit metric too. There are no new methods here."
  },
  {
    "objectID": "posts/post-02/index.html#adapt_diag_e_nuts",
    "href": "posts/post-02/index.html#adapt_diag_e_nuts",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "2.5 adapt_diag_e_nuts",
    "text": "2.5 adapt_diag_e_nuts\nFinally we get to the class the defines the adaptive NUTS sampler with the diagonal metric. This class, adapt_diag_e_nuts, derives from diag_e_nuts. It overrides the transition() method with\nsample transition(sample& init_sample, callbacks::logger& logger) {\n    sample s = diag_e_nuts&lt;Model, BaseRNG&gt;::transition(init_sample, logger);\n\n    if (this-&gt;adapt_flag_) {\n      this-&gt;stepsize_adaptation_.learn_stepsize(this-&gt;nom_epsilon_,\n                                                s.accept_stat());\n\n      bool update = this-&gt;var_adaptation_.learn_variance(this-&gt;z_.inv_e_metric_,\n                                                         this-&gt;z_.q);\n\n      if (update) {\n        this-&gt;init_stepsize(logger);\n\n        this-&gt;stepsize_adaptation_.set_mu(log(10 * this-&gt;nom_epsilon_));\n        this-&gt;stepsize_adaptation_.restart();\n      }\n    }\n    return s;\n  }\nThe actual transition\nsample s = diag_e_nuts&lt;Model, BaseRNG&gt;::transition(init_sample, logger);\nis still performed by calling the implementation of the parent class. This is the implementation defined in base_nuts, because diag_e_nuts does not override it. The other code is for adapting the HMC metric and the integrator step size. To study how this work, we need to find out what stepsize_adaptation_ and var_adaptation_ are. However, we can’t seem to find them in adapt_diag_e_nuts.hpp. So what is again going on?"
  },
  {
    "objectID": "posts/post-02/index.html#class-hierarchy",
    "href": "posts/post-02/index.html#class-hierarchy",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "3.1 Class hierarchy",
    "text": "3.1 Class hierarchy\n\n\n\nThe above diagram explains why sampler objects of class adapt_diag_e_nuts have the stepsize_adaptation_ and var_adaptation_ attributes. The former is for adapting the integrator step size and the latter for adapting the mass matrix diagonal. The metric is adapted in three windows, which are called init_buffer, term_buffer and base_window. The the most abstract base class is base_adaptation, which doesn’t contain any implemented methods."
  },
  {
    "objectID": "posts/post-02/index.html#algorithms",
    "href": "posts/post-02/index.html#algorithms",
    "title": "Understanding the Stan codebase - Part 2: Samplers",
    "section": "3.2 Algorithms",
    "text": "3.2 Algorithms\nLet’s now return to studying the transition() method of the adapt_diag_e_nuts class. When the adapt_flag_ is on (that is, in the warmup phase), each transition will run the code\n  this-&gt;stepsize_adaptation_.learn_stepsize(this-&gt;nom_epsilon_,\n                                                s.accept_stat());\n\n  bool update = this-&gt;var_adaptation_.learn_variance(this-&gt;z_.inv_e_metric_,\n                                                         this-&gt;z_.q);\n\n  if (update) {\n    this-&gt;init_stepsize(logger);\n\n    this-&gt;stepsize_adaptation_.set_mu(log(10 * this-&gt;nom_epsilon_));\n    this-&gt;stepsize_adaptation_.restart();\n  }\n\n3.2.1 learn_stepsize()\nThe learn_stepsize() algorithm is defined in stepsize_adaptation.hpp.\nvoid learn_stepsize(double& epsilon, double adapt_stat) {\n    ++counter_;\n\n    adapt_stat = adapt_stat &gt; 1 ? 1 : adapt_stat;\n\n    // Nesterov Dual-Averaging of log(epsilon)\n    const double eta = 1.0 / (counter_ + t0_);\n\n    s_bar_ = (1.0 - eta) * s_bar_ + eta * (delta_ - adapt_stat);\n\n    const double x = mu_ - s_bar_ * std::sqrt(counter_) / gamma_;\n    const double x_eta = std::pow(counter_, -kappa_);\n\n    x_bar_ = (1.0 - x_eta) * x_bar_ + x_eta * x;\n\n    epsilon = std::exp(x);\n  }\n\n\n3.2.2 learn_variance()\nThe learn_variance() algorithm on the other hand is defined in var_adaptation.hpp\nbool learn_variance(Eigen::VectorXd& var, const Eigen::VectorXd& q) {\n    if (adaptation_window())\n      estimator_.add_sample(q);\n\n    if (end_adaptation_window()) {\n      compute_next_window();\n\n      estimator_.sample_variance(var);\n\n      double n = static_cast&lt;double&gt;(estimator_.num_samples());\n      var = (n / (n + 5.0)) * var\n            + 1e-3 * (5.0 / (n + 5.0)) * Eigen::VectorXd::Ones(var.size());\n\n      if (!var.allFinite())\n        throw std::runtime_error(...);\n\n      estimator_.restart();\n\n      ++adapt_window_counter_;\n      return true;\n    }\n\n    ++adapt_window_counter_;\n    return false;\n  }\nThe interesting thing about this is that it returns true at the end of the three adaptation windows, and otherwise false. This return value (update) determines what happens at the end of the transition.\n\n\n3.2.3 update\n  if (update) {\n    this-&gt;init_stepsize(logger);\n\n    this-&gt;stepsize_adaptation_.set_mu(log(10 * this-&gt;nom_epsilon_));\n    this-&gt;stepsize_adaptation_.restart();\n  }\nThis part is again an interesting algorithmic detail of the adaptation. We see that at the end of each window of metric adaptation, the stepsize adaptation is restarted from 10 times the average stepsize from the previous window. See this thread for some motivation, discussion, and potential problems caused by this approach."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Hobby projects",
    "section": "",
    "text": "Stupid hobby projects."
  },
  {
    "objectID": "projects.html#snake-game",
    "href": "projects.html#snake-game",
    "title": "Hobby projects",
    "section": "Snake game",
    "text": "Snake game\nA simple game using raw Javascript.\n\nPlay\nSource"
  },
  {
    "objectID": "projects.html#darts-simulator",
    "href": "projects.html#darts-simulator",
    "title": "Hobby projects",
    "section": "Darts simulator",
    "text": "Darts simulator\nWhere should I aim given my skill level?\n\nSource"
  },
  {
    "objectID": "projects.html#pace-converter",
    "href": "projects.html#pace-converter",
    "title": "Hobby projects",
    "section": "Pace converter",
    "text": "Pace converter\nUseful for runners.\n\nTry online\nSource"
  },
  {
    "objectID": "software.html",
    "href": "software.html",
    "title": "Software",
    "section": "",
    "text": "I have written many R packages for statistical modeling that use Stan under the hood for fitting the models.\n\nodemodeling: Building and fitting Bayesian ODE models\nlgpr2: Approximate Gaussian process modeling of longitudinal data\nlgpr: Modeling longitudinal data using Gaussian processes"
  }
]